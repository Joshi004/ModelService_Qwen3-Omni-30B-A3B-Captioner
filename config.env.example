# Qwen3-Omni Audio Captioner Service Configuration
# Environment variables for the vLLM service
# Copy this file to config.env and update with your system-specific paths

# Required for Qwen3-Omni compatibility
export VLLM_USE_V1=0

# CUDA Environment
# Update CUDA_HOME to match your CUDA installation path
export CUDA_HOME=/usr/local/cuda-12.9
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# Model Configuration
# Update MODEL_PATH to point to your Qwen3-Omni-30B-A3B-Captioner model directory
MODEL_PATH="/path/to/your/models/qwen3-omni-30b-captioner"
PORT=8003
HOST="0.0.0.0"
DTYPE="bfloat16"

# Performance Settings
# For audio clips up to 30s (recommended), use 32768
MAX_MODEL_LEN=32768
# Update TENSOR_PARALLEL_SIZE based on number of GPUs available
TENSOR_PARALLEL_SIZE=2  # Number of GPUs (e.g., 2x H100 GPUs)
GPU_MEMORY_UTIL=0.95
MAX_NUM_SEQS=8

# Generation Settings (optimal for audio captioning)
TEMPERATURE=0.6
TOP_P=0.95
TOP_K=20
MAX_TOKENS=16384

# Important Notes:
# - This is the Captioner model: audio input only, text output only
# - No text prompts needed - model automatically generates detailed audio captions
# - Optimal audio length: â‰¤ 30 seconds for best detail perception
# - Longer audio clips may diminish detail quality








